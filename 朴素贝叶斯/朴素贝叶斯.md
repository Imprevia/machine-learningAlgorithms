# 朴素贝叶斯（NBM）|机器学习方法（李航）

# 什么是朴素贝叶斯
今天阴天，天气预报说有60%的概率下雨，有20%的概率打雷，下雨后发生打雷的概率只有10%。根据朴素贝叶斯模型就可以求出，打雷后下雨的概率。

朴素贝叶斯模型是一种简单但非常强大的分类器，在垃圾邮件过滤、疾病诊断、文本分类等方面都显示出了巨大的成效。这里的”朴素“是指：设特征之间是**相互独立**的



# 朴素贝叶斯模型

我们收到一封邮件出现“中奖”等词，需要判断它是否垃圾邮件。首先根据以往的邮件统计出在垃圾邮件中出现词“中奖”的概率$P(中奖|垃圾)$，所有邮件中出现垃圾邮件的概论$P(垃圾)$，“中奖”在所有邮件中出现的概论$P(中奖)$，根据朴素贝叶斯模型就求出，邮件中出现“中奖”这个词，是垃圾邮件的概率：
$$
P(垃圾|中奖)=\frac{P(中奖,垃圾)}{P(中奖)}=\frac{P(中奖|垃圾)P(垃圾)}{P(中奖)}
$$
**联合概率：**指包含多个条件且所有条件同时成立的概率。比如上式中的$P(中奖,垃圾)$，包含“中奖”这个词有是垃圾邮件的概率。

**边缘概率：**仅考虑单一事件发生的可能性，而不考虑其它事件发生的可能性。比如上式中的$P(中奖)$和$P(垃圾)$。

**条件概率：**在某一事件发生的情况下，另一事件发生的概率。比如上式中的$P(中奖|垃圾)$

**先验概率：**根据以往的经验，不用采样就可以得到的概率。比如上式中的$P(中奖|垃圾)P(垃圾)P(中奖)$

**后验概率：**指某件事已经发生，依据这件事发生的事实来推断某一个事件在该事件前发生的概率。比如上式中的$P(垃圾|中奖)$



## 公式的推导

由上面我们得到贝叶斯公式的一般形式
$$
P(B|A)=\frac{P(A,B)}{P(A)}=\frac{P(A|B)P(B)}{P(A)}
$$
由公式我们将问题转换为求以下先验概率分布及条件概率分布。

**先验概率分布：**
$$
P(Y=c_k),k=1,2...,k
$$
其中$c_k$代表各个类，$K$为类的总数量，`P(Y=c_k)`代表在训练集中某一类出现的概率。

**条件概率分布：**
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,K
$$
这里指$Y=c_k$的情况下各个特征分别为其某一可取值的概率，上式既有条件也有联合，因此也可称为条件联合概率。

通过条件概率和先验概率（边缘概率）可得到联合概率分布$P(X,Y)$。

我们可以对条件概率分布作条件独立性的假设，虽然该假设可能并不符合实际情况，但在该假设下的一些模型的预测准确率足够用以做出决策，因此可以忽略条件独立性假设造成的误差。

所以在事件独立的情况下，$P(A,B)=P(A)*P(B)$，因此原来的条件概率可转换为多个独立事件的条件概率的连乘：
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$
在此假设下，后验概率的计算用到贝叶斯定理：
$$
P(Y=c_k|X=x)=\frac{P(X=x,Y=c_k)}{P(X=x)}=\frac{P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}=\frac{P(Y=c_k)\prod_{j-1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{P(X=x)}
$$
因为x为新输入实例的特征向量，因此对于Y取任意值，分母不变。我们将后验概率最大的类作为x的类输出，在对比大小时，相同的分母可以忽略，所以最后朴素贝叶斯公式可转换为：
$$
y=\arg \max P(Y=c_k)\prod_{j-1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$
其中，$P(Y=c_k)$即先验概率（边缘概率），$P(X^{(j)}=x^{(j)}|Y=c_k)$即条件概率，可以依据训练集得出。

上式代表计算在某一特征向量的情况下计算该实例为某一类的概率，将最大的概率所对应的类认为是该点的类。



那么接下来的问题就转变为先验概率和条件概率的计算，这两个概率分布的计算都很简单。

先验概率的计算方法：
$$
P(Y=c_k)=\frac{\sum_{j-1}^n I(y_i=c_k)}{N}
$$
其中$N$为训练集中实例的总数，$I$为指示函数，表示若括号中条件成立则取1，否则取0。比如训练集一共有100个实例，其中分类为A的实例有50个，则$P(Y=A)=50/100=1/2$。

条件概率的计算方法，原理同上：
$$
P(X^{(j)}=a^{(j)}|Y=c_k)=\frac{\sum_{i-1}^n I(x_i^{(j)}=a^{(j)}|y_i=c_k)}{\sum_{i-1}^nI(y_i=c_k)}
$$

# 贝叶斯估计

如果新出现的实例中有一个特征的取值在训练集中从来没有出现过，根据朴素贝叶斯公式中的$\prod_{j-1}^n P(X^{(j)}=x^{(j)}|Y=c_k)$这一连乘项中的一个值为0，即 $\prod_{j-1}^n P(X^{(j)}=x^{(j)}|Y=c_k)=0$ ，最终导致$Y$取任意情况下后验概率皆为0，无法对新输入实例进行分类。

为了解决这一问题引入了贝叶斯估计

贝叶斯估计的先验概率为：
$$
P_\lambda(Y=c_k)=\frac{\sum_{j-1}^n I(y_i=c_k)+\lambda}{N+K\lambda}
$$
条件概率为：
$$
P_\lambda(X^{(j)}=a^{(j)}|Y=c_k)=\frac{\sum_{i-1}^n I(x_i^{(j)}=a^{(j)}|y_i=c_k)+_\lambda}{\sum_{i-1}^nI(y_i=c_k)+S_j\lambda}
$$
式中$\lambda≥0$，当$\lambda=1$，则称为拉普拉斯平滑（一般都使$\lambda=1$）。$K$为训练集中Y的类的数量，$S_j$为特征$x^{(j)}$可取值的数量。

也就是说，对于先验概率，拉普拉斯平滑在其分子加1，在分母加上类别的数量。对于条件概率，则是在分子加1，在分母加上该特征的可取值的数量。

尽管$\lambda$会让后验概率与未加入$\lambda$之前有所变化，但是在数据量非常大的情况下，$\lambda$的加入所引起的后验概率的变化是在可接受范围的，对最终的分类结果影响可以忽略不计。
