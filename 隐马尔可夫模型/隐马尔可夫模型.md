# 隐马尔可夫模型

了解隐马尔可夫模型前，先说一下**马尔可夫性**：在某个时刻$t\geq 1$的随机变量$X_t$与前一个时刻的随机变量$X_{t-1}$之间有条件分布$P(X_t|X_{t-1})$， 如果$X_t$只依赖于$X_{t-1}$ ，而不是依赖于过去的随机变量 $\{X_i,X_2, \cdots ,X_{t-2}\}$，这一性质称为马尔可夫性，即：
$$
P(X_t|X_i,X_2, \cdots ,X_{t-1}) = P(X_t|X_{t-1}),t=1,2, \cdots
$$


马尔可夫性，可以通俗的理解为：现在决定未来，而与过去无关。



**马尔科夫链**：在t时刻发生的事与t-1时刻发生的事件有关，即t时刻事件与t-1时刻事件不是互相独立的，并且t时刻发生的事也会对t+1时刻的事产生影响，我们可以认为上述的多个按时间顺序发生的非独立事件形成了马尔可夫链。

**马尔可夫过程**：上面的是离散时间马尔可夫链，因为其时间是离散的，对于时间连续的马尔可夫链，我们称之为马尔可夫过程。



# 隐马尔可夫模型

设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合：
$$
Q=\{q_1,q_2, \cdots ,q_M\},V=\{v_1,v_2, \cdots, v_1 \}
$$
$N$是可能的状态数，$M$是可能的观测数。

$I$是长度为$T$的状态序列，$O$是对应的观测序列：
$$
I=(i_1,i_2, \cdots, i_T),O=(o_1,o_2, \cdots, o_T)
$$
$A$是状态转移概率矩阵：
$$
A=[a_{ij}]_{N \times N }
$$
其中：
$$
a_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2, \cdots, N;j=1,2, \cdots, N
$$
是在时刻t处于状态$q_i$的条件下在时刻$t+1$转移到状态$q_j$的概率。



$B$是观测概率矩阵：
$$
B=[b_j(k)]_{N \times M}
$$
其中：
$$
b_j(k)=P(o_t=v_k|i_j=q_j),k=1,2, \cdots,M;j=1,2,\cdots,N
$$
是在时刻$t$处于状态$q$，的条件下生成观测$v_k$的概率。
$\pi$是初始状态概率向量：
$$
\pi = (\pi_i)
$$
其中，
$$
\pi_i=P(i_i=q_i),i=1,2,\cdots,N
$$
是时刻$t=1$处于状态$q$，的概率.
隐马尔可夫模型由初始状态概率向量$\pi$、状态转移概率矩阵$A$和观测概率矩
阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔可夫模型$\lambda$可
以用三元符号表示，即
$$
\lambda = (A,B,\pi)
$$
$A,B,\pi$称为隐马尔可夫模型的三要素。



# 隐马尔可夫链模型的假设

1. 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关：

$$
P(i_t|i_{t-1},o_{t-1}, \cdots , i_1, o_i) = P(i_i|i_{i-1}), t=1,2, \cdots,T
$$

2. 观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：
   $$
   P(o_i|i_T,oT,i_{T-1},o_{T-1}, \cdots, i_{t+1},o_{t+1},i_{t},o_t,i_{t-1},o_{t-1}, \cdots ,i_1,o_1) = P(o_t|i_t)
   $$
   

# 隐马尔可夫模型学习的基本步骤

1. 计算概率。给定模型 $\lambda = (A,B,\pi)$ 和观测序列$O=(o_1,o_2, \cdots, o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。

2. 学习问题。已知观测序列，估计模型参数，使得在该模型下观测序列概率最大，即用极大似然估计的方法估计参数。

3. 预测问题，也称为解码问题。已知模型和观测序列，求对给定观测序列条件概率P(I|O）最大的状态序列I。即给定观测序列，求最有可能的对应的状态序列。

   

# 概率计算算法

书中介绍了三种方法：

* 直接计算法
* 向前算法
* 向后算法



## 直接计算法

最直接的方法是按概率公式直接计算.通过列举所有可能的长度为T的状态序列$I=(i_1,i_2, \cdots, i_T)$，求各个状态序列$I$与观测序列$O=(o_1,o_2, \cdots, o_T)$的联合概率$P(O,I|\lambda)$，然后对所有可能的状态序列求和，得到$P(O|\lambda)$。



## 向前算法

前向算法是一种使用前向概率来达到使用较少的计算量就能够算出$P(O|\lambda)$的算法，首先我们需要定义什么是前向概率。

前向概率为给定隐马尔可夫模型$\lambda$，在时刻$t$，观测序列为$o_1, o_2, \cdots, o_t$且状态为$q_i$的概率：
$$
a_t(i)=P(o_1, o_2, \cdots, o_t， i_t=q_i| \lambda)
$$
通过前向概率推导得到$P(O|\lambda)$的步骤有三步：

1. 初值：

$$
a_1(i)=\pi_ib_i(o_1),i=1,2, \cdots,N
$$

$α_1(i)$为时刻$t=1$时，观测序列为$o_1$，且状态为$q_i$的概率。等式右边的$\pi_i$为初始状态概率向量中$q_i$的概率，$bi(o1)$表示状态为$qi$时，观测为$o_1$的概率。

记住，此时状态$q_i$是固定的，我们在最后将对所有的状态进行加和。

2. 递推，对$t=1,2, \cdots, T-1$:
   $$
   a_{t+1}(i)=[\sum_{j=1}^Na_t(j)a_{ji}]b_i(o_{t+1}), i=1,2, \cdots,N
   $$
   

计算到时刻$t+1$部分观测序列为$o_1,o_2, \cdots, o_t$且在时刻$t+1$处于状态$q_i$的前向概率。

公式中方括弧里，既然心$a_t(j)$是到时刻$t$观测到$o_1,o_2, \cdots, o_t$并在时刻$t$处于状态$q_j$的前向概率，那么乘积$a_t(j)a_{ji}$就是到时刻t观测到$o_1,o_2, \cdots, o_t$并在时刻$t$处于状态$q_j$而在时刻$t+1$到达状态$q_i$的联合概率。对这个乘积在时刻t的所有可能的$N$个状态$q_j$求和，其结果就是到时刻$t$观测为$o_1,o_2, \cdots, o_t$并在时刻￥t+1￥处于状态$q_i$的联合概率。方括弧里的值与观测概率$b_i(o_{t+1})$的乘积恰好是到时刻$t+1$观测到$o_1,o_2, \cdots, o_t, o_{t+1}$并在时刻$t+1$处于状态$q_i$的前向概率$a_{t+1}(i)$。



3. 终止
   $$
   P(O|\lambda) = \sum_{i=1}^Na_T(i)
   $$
   

其中$α_T(i)$为在最后时刻，整个观测序列已知的情况下，状态为$q_i$的概率。此时再将在最后时刻，状态为所有可能状态的概率相加，即得到在模型已知情况下，得到已知观测序列的概率。

计算时刻$t+1$的某一个状态时，已经将之前所有的状态的可能性相加了，也就是递推公式中括号内的求和项，如下图所示：

![image-20240227201931404](.\images\10.1.png)

上图只考虑了时刻$t+1$的一个状态，如果我们将所有的状态都考虑进去，则是：

![image-20240227203304466](.\images\10.2.png)

我们将图中的结构叫做观测序列路径结构。这种结构在每一次计算时，可以直接引用前一个时刻的计算结果，如此就避免了直接计算法中的重复计算。前向算法的时间复杂度为$O(T*N^2)$，远小于直接计算法的$O(T*N^T)$。

## 后向算法

给定隐马尔可夫模型$\lambda$，在时刻$t$状态为$q_i$的条件下，从$t+1$到$T$的部分观测序列为$o_{t+1}, o_{t+2}, \cdots, o_T$的概率为后向概率：
$$
\beta_t(i)=P($o_{t+1}, o_{t+2}, \cdots, o_T|i_t=q_i,\lambda)
$$
通过后向概率推导$P(O|\lambda)$同样有三步。

首先，我们对最终时刻的所有状态$q_i$规定$\beta_t(i)=1, i=1,2, \cdots , N$。

接着，对$t=T-1,T-2, \cdots , 1$：
$$
\beta_t(i)= = \sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}(j),i=1,2, \cdots,N
$$
为了计算在时刻$t$状态为$q_i$，条件下时刻$t+1$之后的观测序列为$o_{t+1},o_{t+2}, \cdots , o_T$的后向概率$\beta_t(i)$，只需考虑在时刻$t+1$所有可能的$N$个状态$q_j$，的转移概率（即$a_{ij}$项），以及在此状态下的观测$o_{t+1}$的观测概
率（即$b_j(o_{t+1})$项），然后考虑状态$q_j$，之后的观测序列的后向概率（即$\beta_{t+1}(j)$项)。

最后：
$$
P(O \mid \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}(i) 
$$


# 学习算法

隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与无监督学习实现。



## 监督学习方法

假设已给训练数据包含S个长度相同的观测序列和对应的状态序列${(O_1,I_1), (O_2,I_2), \cdots , (O_s,I_s)}$，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。具体方法如下：

1. 转移概率$a_{ij}$的估计

   设样本中时刻$t$处于状态$i$，且时刻$t+1$转移到状态j的频数为$A_{ij}$，那么状态转移概率$a_{ij}$的估计是：
   $$
   \hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N} A_{ij}}， i=1, 2， \cdots, N ; j=1, 2, \cdots, N 
   $$
   

2. 观测概率$b_j(k)$的估计

​		设样本中状态为$j$并观测为$k$的频数是$B_{jk}$，那么状态为$j$观测为$k$的概率$b_j(k)$的估计是：
$$
\\\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, j=1,2, \cdots, N ; k=1,2, \cdots, M 
$$

3. 初始状态概率$\pi_i$的估计$\hat\pi_i$为$S$个样本中初始状态为$q_i$的频率。

   在监督学习方法中，对三个参数的估计很简单，直接按照上面三个式子计算就可以了。但是现实情况中往往是没有标记的，其状态序列往往是隐变量，此时就需要用到无监督学习的方法。

## Baum-Welch算法