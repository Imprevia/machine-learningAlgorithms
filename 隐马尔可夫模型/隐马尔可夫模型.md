# 隐马尔可夫模型

了解隐马尔可夫模型前，先说一下**马尔可夫性**：在某个时刻$t\geq 1$的随机变量$X_t$与前一个时刻的随机变量$X_{t-1}$之间有条件分布$P(X_t|X_{t-1})$， 如果$X_t$只依赖于$X_{t-1}$ ，而不是依赖于过去的随机变量 $\{X_i,X_2, \cdots ,X_{t-2}\}$，这一性质称为马尔可夫性，即：
$$
P(X_t|X_i,X_2, \cdots ,X_{t-1}) = P(X_t|X_{t-1}),t=1,2, \cdots
$$


马尔可夫性，可以通俗的理解为：现在决定未来，而与过去无关。



**马尔科夫链**：在t时刻发生的事与t-1时刻发生的事件有关，即t时刻事件与t-1时刻事件不是互相独立的，并且t时刻发生的事也会对t+1时刻的事产生影响，我们可以认为上述的多个按时间顺序发生的非独立事件形成了马尔可夫链。

**马尔可夫过程**：上面的是离散时间马尔可夫链，因为其时间是离散的，对于时间连续的马尔可夫链，我们称之为马尔可夫过程。



# 隐马尔可夫模型

设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合：
$$
Q=\{q_1,q_2, \cdots ,q_M\},V=\{v_1,v_2, \cdots, v_1 \}
$$
$N$是可能的状态数，$M$是可能的观测数。

$I$是长度为$T$的状态序列，$O$是对应的观测序列：
$$
I=(i_1,i_2, \cdots, i_T),O=(o_1,o_2, \cdots, o_T)
$$
$A$是状态转移概率矩阵：
$$
A=[a_{ij}]_{N \times N }
$$
其中：
$$
a_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2, \cdots, N;j=1,2, \cdots, N
$$
是在时刻t处于状态$q_i$的条件下在时刻$t+1$转移到状态$q_j$的概率。



$B$是观测概率矩阵：
$$
B=[b_j(k)]_{N \times M}
$$
其中：
$$
b_j(k)=P(o_t=v_k|i_j=q_j),k=1,2, \cdots,M;j=1,2,\cdots,N
$$
是在时刻$t$处于状态$q$，的条件下生成观测$v_k$的概率。
$\pi$是初始状态概率向量：
$$
\pi = (\pi_i)
$$
其中，
$$
\pi_i=P(i_i=q_i),i=1,2,\cdots,N
$$
是时刻$t=1$处于状态$q$，的概率.
隐马尔可夫模型由初始状态概率向量$\pi$、状态转移概率矩阵$A$和观测概率矩
阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔可夫模型$\lambda$可
以用三元符号表示，即
$$
\lambda = (A,B,\pi)
$$
$A,B,\pi$称为隐马尔可夫模型的三要素。



# 隐马尔可夫链模型的假设

1. 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关：

$$
P(i_t|i_{t-1},o_{t-1}, \cdots , i_1, o_i) = P(i_i|i_{i-1}), t=1,2, \cdots,T
$$

2. 观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：
   $$
   P(o_i|i_T,oT,i_{T-1},o_{T-1}, \cdots, i_{t+1},o_{t+1},i_{t},o_t,i_{t-1},o_{t-1}, \cdots ,i_1,o_1) = P(o_t|i_t)
   $$
   

# 隐马尔可夫模型学习的基本步骤

1. 计算概率。给定模型 $\lambda = (A,B,\pi)$ 和观测序列$O=(o_1,o_2, \cdots, o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。

2. 学习问题。已知观测序列，估计模型参数，使得在该模型下观测序列概率最大，即用极大似然估计的方法估计参数。

3. 预测问题，也称为解码问题。已知模型和观测序列，求对给定观测序列条件概率P(I|O）最大的状态序列I。即给定观测序列，求最有可能的对应的状态序列。

   

# 概率计算算法

书中介绍了三种方法：

* 直接计算法
* 向前算法
* 向后算法



## 直接计算法

最直接的方法是按概率公式直接计算.通过列举所有可能的长度为T的状态序列$I=(i_1,i_2, \cdots, i_T)$，求各个状态序列$I$与观测序列$O=(o_1,o_2, \cdots, o_T)$的联合概率$P(O,I|\lambda)$，然后对所有可能的状态序列求和，得到$P(O|\lambda)$。



## 向前算法

前向算法是一种使用前向概率来达到使用较少的计算量就能够算出$P(O|\lambda)$的算法，首先我们需要定义什么是前向概率。

前向概率为给定隐马尔可夫模型$\lambda$，在时刻$t$，观测序列为$o_1, o_2, \cdots, o_t$且状态为$q_i$的概率：
$$
a_t(i)=P(o_1, o_2, \cdots, o_t， i_t=q_i| \lambda)
$$
