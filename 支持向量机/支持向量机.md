# 支持向量机

支持向量机，可以看着是升级版的感知机，与感知机相比。他们都是找到一个超平面对数据集进行分割，区别在于，感知机模型得到的超平面空间中可以有无穷个超平面，但支持向量机仅含有一个，这一个超平面与样本点的间隔是最大化的。

支持向量机学习方法包含三种模型：

* 线性可分支持向量机，要求训练集线性可分，通过硬间隔最大化得到超平面。
* 线性支持向量机，要求训练集近似线性可分，通过软间隔最大化获得超平面
* 非线性支持向量机，训练集线性不可分，可通过使用核函数将线性不可分的训练集转换为线性可分的数据集，并通过软间隔最大化获得超平面。

# 线性可分支持向量机

对于线性可分的数据集，学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类。分离超平面的法向量指向的一侧为正类，另一侧为负类。

并且要求这个分离超平面距离最近的两个点的距离之和最大，这个分离超平面被称为间隔最大分离超平面。

线性可分支持向量机的数学模型为：
$$
f(x)=sign(w^**x+b^*)
$$



其中$w^**x+b^* = 0$就是间隔最大分离超平面。

我们所需要求得的模型就是这个间隔最大的分离超平面。

## 最大间隔法

求一个最大间隔分离超平面，可以用约束最优化问题的形式来表示：
$$
\begin{aligned}
&max_{w,b}\gamma	\\
&s.t. \quad y_i\left( \frac{w}{||w||}*x_i+\frac{b}{||w||} \right)\geq \gamma,\quad i=1,2,...,N
\end{aligned}
$$
我们可以通过几何间隔和函数间隔的关系代入上面的公式：
$$
\begin{aligned}
& max_{w,b}\frac{\hat{\gamma}}{||w||} \\
&s.t.\quad y_i\left( w*x_i+b \right)\geq \hat{\gamma},\quad i=1,2,...,N
\end{aligned}
$$
因为函数间隔对w和b按比例改变，函数间隔也按此比例改变的特性，w和b的改变并不会影响上式中的目标函数的优化和约束条件，因此，我们可以对函数间隔随意取值，为了方便，我们取$\hat{\gamma}=1$