# 支持向量机

支持向量机，可以看着是升级版的感知机，与感知机相比。他们都是找到一个超平面对数据集进行分割，区别在于，感知机模型得到的超平面空间中可以有无穷个超平面，但支持向量机仅含有一个，这一个超平面与样本点的间隔是最大化的。

支持向量机学习方法包含三种模型：

* 线性可分支持向量机，要求训练集线性可分，通过硬间隔最大化得到超平面。
* 线性支持向量机，要求训练集近似线性可分，通过软间隔最大化获得超平面
* 非线性支持向量机，训练集线性不可分，可通过使用核函数将线性不可分的训练集转换为线性可分的数据集，并通过软间隔最大化获得超平面。

# 线性可分支持向量机

对于线性可分的数据集，学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类。分离超平面的法向量指向的一侧为正类，另一侧为负类。

并且要求这个分离超平面距离最近的两个点的距离之和最大，这个分离超平面被称为间隔最大分离超平面。

线性可分支持向量机的数学模型为：
$$
f(x)=sign(w^**x+b^*)
$$



其中$w^**x+b^* = 0$就是间隔最大分离超平面。

我们所需要求得的模型就是这个间隔最大的分离超平面。



## 函数间隔和几何间隔

### 函数间隔

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w*x+b=0$确定的情况下，$|w*x+b|$能够相对地表示点x距离超平面的远近。而$w*x+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。因为在二分类问题中，$y$要么为1，要么为-1，所以可用$y(w*x+b)$来表示分类的正确性及确信度，这就是函数间隔的概念。

定义超平面关于样本点$(x_i,y_i)$的函数间隔为：
$$
\hat{\gamma}_i=y_i(w \cdot x_i+b)
$$
定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔的最小值：
$$
\hat{\gamma} = \min_{i=1,...,N}\hat{\gamma}_i
$$
但是选择分离超平面时，只有函数间隔还不够。因为只要成比例改变$w$和$b$超平面并没有改变，函数间隔却改变了。

### 几何间隔

因为函数间隔无法确定$w$和$b$，所以可以对$w$施加约束，如规范化$||w||=1$，这样函数间隔就成为几何间隔。

定义超平面关于样本点$(x_i,y_i)$的几何间隔为：
$$
\gamma_i=\quad y_i\left( \frac{w}{||w||} \cdot x_i+\frac{b}{||w||} \right)
$$


定义超平面（w,b）关于训练数据集T的几何间隔为超平面（w,b）关于T中所有样本点（xi,yi）的几何间隔的最小值：
$$
\gamma=\min_{i=1,...,N}\gamma_i
$$


几何间隔与函数间隔的关系：
$$
\gamma=\frac{\hat{\gamma}}{||w||}
$$


## 最大间隔法

求一个最大间隔分离超平面，可以用约束最优化问题的形式来表示：
$$
\begin{aligned}
& \max_{w,b}\gamma	\\
&s.t. \quad y_i\left( \frac{w}{||w||} \cdot x_i+\frac{b}{||w||} \right)\geq \gamma,\quad i=1,2,...,N
\end{aligned}
$$
我们可以通过几何间隔和函数间隔的关系代入上面的公式：
$$
\begin{aligned}
& \max_{w,b}\frac{\hat{\gamma}}{||w||} \\
&s.t.\quad y_i\left( w \cdot  x_i+b \right)\geq \hat{\gamma},\quad i=1,2,...,N
\end{aligned}
$$
因为函数间隔对w和b按比例改变，函数间隔也按此比例改变的特性，w和b的改变并不会影响上式中的目标函数的优化和约束条件，因此，我们可以对函数间隔随意取值，为了方便，我们取$\hat{\gamma}=1$

另外，我们可以将最大化问题转换为最小化问题，即将 $\max \frac{1}{||w||}$转换为 $\min ||w||$，这样得到的问题为：
$$
\begin{aligned}
& \min_{w,b}||w|| \\
&s.t.\quad y_i\left( w \cdot  x_i+b \right)-1\geq 0,\quad i=1,2,...,N
\end{aligned}
$$
可以使用拉格朗日乘数法求解。

同时为了之后的计算方便，我们可以将问题转变为：
$$
\begin{aligned}
& \min_{w,b}\frac12||w||^2 \\
&s.t.\quad y_i\left( w \cdot  x_i+b \right)-1\geq 0,\quad i=1,2,...,N
\end{aligned}
$$
因为$||w||\geq 0$，因此平方后再乘以一个系数不会对最小化问题产生影响。

上述的问题是一个凸二次规划问题。

现在，我们只需解答上述问题，得到最优特征向量$w^*$和最优偏置$b^*$，就可以得到最大间隔分离超平面和决策函数。问题在于如何求解该问题。



## 对偶算法

可以参考最大熵模型使用过拉格朗日乘数法和对偶法解决约束条件下的最优化问题。

这里和最大熵模型使用的拉格朗日乘子法的不同在于，最大熵模型中的约束条件是等式约束条件，而这里是不等式约束条件，对于这样的情况，可以应用KKT条件去求得最优值。

构建拉格朗日函数：
$$
L(w,b,a)=\frac12||w||^2-\sum_{i=1}^Na_i(y_i(w \cdot x_i+b)-1)=\frac12||w||^2-\sum_{i=1}^Na_i y_i(w \cdot x_i+b)+\sum_{i=1}^Na_i
$$
其中$α=(α_1,α_2,…,α_N)$为拉格朗日乘子向量。

原始问题为：
$$
\min_{w,b}\max_{a}L(w,b,a)
$$
其对偶问题为：
$$
\max_a\min_{w,b}L(w,b,a)
$$
现在求对偶问题的内部最小化。

我们对w和b分别求偏导，并令偏导数等于0。
$$
\nabla_wL(w,b,a)=w-\sum_{i=1}^Na_iy_ix_i=0 \\
\nabla_b L(w,b,a)=-\sum_{i=1}^Na_iy_i=0
$$
得到：
$$
w=\sum_{i=1}^Na_iy_ix_i \\
\sum_{i=1}^Na_iy_i=0
$$
将上面的结论代入到拉格朗日函数之中：
$$
\begin{align*}
\min_{w,b}L(w,b,a)&=\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)-\sum_{i=1}^Na_iy_i\left(\left(\sum_{j=1}^Na_jy_jx_j\right) \cdot x_i+b \right)+\sum_{i=1}^Na_i \\
&=\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)-\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)-\sum_{i=1}^Na_iy_ib+\sum_{i=1}^Na_i \\
&=-\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)+\sum_{i=1}^Na_i
\end{align*}
$$
接下来求对偶问题中的外部极大化问题：

我们可以通过在右侧乘以一个-1，将最大化问题转换为求最小化的问题：
$$
\begin{aligned}
& \min_a\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)-\sum_{i=1}^Na_i \\
&s.t.\quad \sum_{i=1}^Na_iy_i=0 \\
& a_i \geq 0,i=1,2,...,N
\end{aligned}
$$
通过约束条件我们可以得到α的各个分量之间的关系，代入原式后，对$α_i$求偏导，并使偏导数为0，求得α向量。



## 利用KKT求最优w和b

线性可分支持向量机中的原问题和对偶问题互相之间是强对偶关系，而KKT是强对偶关系的充要条件。

因为原始问题和对偶问题是强对偶关系，所以一定满足KKT条件：
$$
\nabla_wL(w^*,b^*,a^*)=w^*-\sum_{i=1}^Na_i^*y_ix_i=0 \tag{1}
$$

$$
\nabla_b L(w^*,b^*,a^*)=-\sum_{i=1}^Na_i^*y_i=0 \tag{2}
$$

$$
a_i^*(y-i(x_i+b^*)-1)=0,i=1,2,...,N \tag{3}
$$

$$
y_i(w^* \cdot x_i + b^*)-1 \geq0,i=1,2,...N \tag{4}
$$

$$
a_i^* \geq 0,i=1,2,...N \tag{5}
$$

在上面的五个KKT条件中，上标星号表示为最优解。等式(1)和等式(2)是我们之前在内部极小化拉格朗日函数的时候求得的，剩下的三个式子是KKT条件需要满足的。

那么根据等式(1)，我们可以很容易的得到：
$$
w^*=\sum_{i}a_i^*y_ix_i
$$
主要来看b，主要关注于式(3)，(4)和(5)。

当$a_i^* \geq 0$时， $y_i(w^* \cdot x_i + b^*)-1 = 0$，此时 $|w^* \cdot x_i + b^*|=1 $。

而函数距离就是通过$|w·x+b|$来得到。

也就是说当$a_i^* \geq 0$时，我们找到了距离超平面的函数距离最小的样本点，这样的样本点我们称之为支持向量。

而当$a_i^* = 0$时， $y_i(w^* \cdot x_i + b^*)-1$ 可以是大于等于0，即这些样本点与超平面的函数距离大于支持向量，同时因为 $a_i^* = 0$ ，说明这些样本点在我们的模型中并未被考虑进去，也就是说它们对最大距离分离超平面的确定没有关系。

于是，存在一个样本点$(x_i,y_i)$使$a_i^* \geq 0$ ，根据  $y_i(w^* \cdot x_i + b^*)-1=0$ ，可以得到：
$$
y_i^2(w^* \cdot x_i + b^*)=y_i\rightarrow b^*=y_i-w^* \cdot x_i \rightarrow b^*=y_i-\sum_ia_i^*y_ix_i \cdot x_j
$$


现在我们得到了最优$w^*$和$b^*$，同时$α_i^*$也在上一节通过求偏导得到，于是也就得到了最大距离分离超平面和决策函数：
$$
\sum_{i=i}^Na_i^*y_i(x \cdot x_i)+b^*=0 \\
f(x)=sign\left( \sum_{i=1}^Na_i^*y_i(x \cdot x_i)+b^* \right)
$$


# 线性支持向量机

在线性不可分的数据集中，一个超平面不可能把所有样本点都正确分类，因此至少存在一个样本点$(x_k,y_k)$，使$(yk)(w·(xk)+b)<0$，所以$(y_k)(w·(x_k)+b)-1<0$，与不等式约束矛盾。

在线性支持向量机中，我们将线性可分支持向量机的硬间隔最大化改为软间隔最大化，就可以在近似线性可分的数据集中运用支持向量机了。

所谓近似线性可分的数据集，就是训练数据中有一些样本点，将这些样本点去除后就可以得到线性可分的数据集。这些样本点，我们称其为特异点。

特异点不满足函数间隔大于等于1的约束条件，这时，我们对每个样本点引入一个松弛变量 $\xi \geq 0$ ，使函数间隔加上松弛变量大于等于1，于是约束条件变为：
$$
y_i(w \cdot x_i +b) \geq 1- \xi
$$
同时，将原来的目标函数变为：
$$
\frac12||w||^2+C\sum_{i=1}^N\xi_i
$$
其中$C>0$为惩罚参数，C值大时对误分类的惩罚增大，反之，对误分类的惩罚减小。于是，最小化上式可以理解为，使前一项尽量小，即间隔尽量大，并且使误分类点的数量尽量小，也就是让后一项尽量小。参数C在两者之间有调和作用。这种间隔最大化的方法，称为软间隔最大化。

比较直观的理解松弛变量，就是松弛变量的加入不再要求特异点与分离超平面的函数距离大于等于1，但与此同时也需要给予最小化式子惩罚，以此来限制过于松弛的情况，而系数C则是用来调节间隔的软硬程度。

这样，我们就可以和训练线性可分的数据集一样来训练近似线性可分的数据集了。于是我们得到的问题为：
$$
\begin{aligned}
& \min_{w,b,\xi}\frac12||w||^2+C\sum_{i=1}^N\xi_i \\
&s.t.\quad y_i(w \cdot x_i+b) \geq 1-\xi_i,i=1,2,...,N \\
& \xi_i \geq 0,i=1,2,...,N
\end{aligned}
$$
从上面的问题中可以看出来，线性可分支持向量机是线性支持向量机中的一种特殊情况，如果数据集中没有特异点，则对于每个样本点来说松弛变量都为0，则近似线性可分转变为线性可分的情况。

线性支持向量机最后得到的超平面和决策函数为：
$$
w^* \cdot x_i + b^* = 0 \\
f(x)=sign(w^* \cdot x + b^*)
$$

## 对偶算法

首先建立拉格朗日函数：
$$
L(w,b,\xi,a,\mu)=\frac12||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^Na_i(y_i(w \cdot x_i+b)-1+\xi_i)-\sum_{i=1}^N\mu_i\xi_i
$$
其中拉格朗日乘数$α_i≥0$，$\mu_i≥0$。

原始问题为：
$$
\min_{w,b,\xi}\max_{a,\mu}L(w,b,\xi,a,\mu)
$$
对偶问题为：
$$
\max_{a,\mu}\min_{w,b,\xi}L(w,b,\xi,a,\mu)
$$
首先，对于内部极小化问题，对求偏导，并使偏导数等于0：
$$
\nabla_wL(w,b,\xi,a,\mu)=w-\sum_{i=1}^Na_iy_ix_i=0 \tag{1}
$$

$$
\nabla_bL(w,b,\xi,a,\mu)=-\sum_{i=1}^Na_iy_i=0 \tag{2}
$$

$$
\nabla_\xi L(w,b,\xi,a,\mu)=C-a_i-\mu_i=0 \tag{3}
$$

得到：
$$
w=\sum_{i=1}^Na_iy_ix_i
$$

$$
\sum_{i=1}^Na_iy_i=0
$$

$$
C-a_i-\mu_i=0
$$

再将上面三个等式代回至拉格朗日函数中：
$$
\min_{w,b,\xi}L(w,b,\xi,a,\mu)=-\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)+\sum_{i=1}^Na_i
$$
再解对偶问题的外部极大化问题，得到问题：
$$
\max_a-\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)+\sum_{i=1}^Na_i \\
\sum_{i=1}^Na_iy_i=0 \\
s.t.\quad C-a_i-\mu_i=0 \\
\mu_i \geq 0,i=1,2,...,N
$$
在约束条件中，我们可以利用后面三个式子将μi消去：因为$\mu_i=C-α_i$，且$μ_i\geq 0$，所以$C-α_i \geq 0$，又因为$α_i\geq 0$，所以$C \geq  α_i \geq  0$。

并且我们将最大化问题通过乘以一个-1转换为最小化问题。

于是上述问题转变为：
$$
\begin{aligned}
&\min_a\frac12\sum_{i=1}^N\sum_{j=1}^Na_ia_jy_iy_j(x_i \cdot x_j)-\sum_{i=1}^Na_i \\
&s.t. \quad \sum_{i=1}^Na_iy_i=0 \\
&0\leq a_i \leq C,i=1,2,...,N
\end{aligned}
$$
此时根据约束条件中α的各个分量的关系，代入极小化的式子中，并对每一个αi求偏导，并令偏导数为0，即可得到α向量。



## 线性支持向量机的KKT条件

与线性可分支持向量机相似，由于原始问题和对偶问题之间满足强对偶关系，因此可以利用KKT条件对$w^*$和$b^*$进行求解。以下是线性支持向量机的KKT条件：
$$
\nabla_wL(w^*,b^*,\xi^*,a^*,\mu^*)=w^*-\sum_{i=1}^Na_i^*y_ix_i=0 \tag{1}
$$

$$
\nabla_bL(w^*,b^*,\xi^*,a^*,\mu^*)=-\sum_{i=1}^Na_i^*y_i=0 \tag{2}
$$

$$
\nabla_\xi L(w^*,b^*,\xi^*,a^*,\mu^*)=C-a_i^*-\mu_i^*=0 \tag{3}
$$

$$
a_i^*(y_i(w^* \cdot x_i +b^*)-1+\xi_i^*)=0 \tag{4}
$$

$$
\mu_i^*\xi_i^*=0 \tag{5}
$$

$$
y_i(w^* \cdot x_i + b^*)-1+\xi_i^* \geq 0 \tag{6}
$$

$$
\xi_i^* \geq 0 \tag{7}
$$

$$
a_i^* \geq 0 \tag{8}
$$

$$
\mu_i^* \geq 0 ,i=1,2,...,N \tag{9}
$$

若存在$α_m=C$，则根据式(3)可得，$μ_m=0$，根据式(4)可得 $y_m(w^* \cdot x_m + b^*)-1+\xi_m^* \geq 0$ ，则无法确定$b^*$，根据式(5)可得$\xi_m^* > 0$ ，可知该点为特异点。

若存在$α_k=0$，则根据式(3)可得，$μ_k=C$，根据式(4)可得 $y_k(w^* \cdot x_k + b^*)-1+\xi_k^* > 0$ ，则无法确定$b^*$，根据式(5)可得$ \xi_k^*=0$ ，所以 $y_k(w^* \cdot x_k + b^*)-1 > 0$ ，可知该点为普通样本点。

若存在$0<α_j<C$，则根据式(3)可得，$μ_j \ne 0$，根据式(4)可得$y_k(w^* \cdot x_k + b^*)-1+\xi_k^* = 0$  ，且根据式(5)可得$ \xi_k^*=0 $，所以 $y_k(w^* \cdot x_k + b^*)-1 = 0$ ，样本点落在间隔边界上，可确定$b^*$。

由此可得：
$$
b^*=y_i-\sum_{i=i}^Ny_ia_i^*(x_i \cdot x_j)
$$
由式(1)可得：
$$
w^*=\sum_{i=1}^Na_i^*y_ix_i
$$
再代入之前求得的$α^*$，可得线性支持向量机的分离超平面和分类决策函数。

## 线性支持向量机的支持向量

我们将$a_i>0$时对应的样本点称为线性支持向量机的支持向量。

![image-20240219181734092](.\images\7.5.png)

软间隔的支持向量$x_i$或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。



## 合页损失函数

线性支持向量机还可以通过最小化合页损失函数来获得分离超平面。

合页损失函数为：
$$
\sum_{i=1}^N[1-y_i(x \cdot x_i+b)]_++\lambda||w||^2
$$
其中第一项是经验损失，下标+表示：
$$
[Z]_+=
\begin{cases}
z, \quad z>0 \\
0, \quad z \leq 0
\end{cases}
$$
放在SVM的情况中， $1-y_i(x \cdot x_i+b) > 0$ 时， $y_i(x \cdot x_i+b)<1$，函数距离小于1，该样本点要么是被正确分类的但位于分离超平面和间隔边界之间，要么是特异点，损失就是 $1−y_i(x \cdot x_i+b)$，若$1−y_i(x \cdot x_i+b)<0$  ，代表该样本点被正确分类且在间隔边界之外，损失为0。

合页损失函数的第二项为正则化项，是用来防止过拟合的。

现在证明为何原始最优化问题等价于合页损失函数最小化。

原始最优化问题：
$$
\begin{aligned}
&\min_{w,b,\xi}\frac12||w||^2+C\sum_{i=1}^N\xi_i \\
&s.t. \quad y_i(w \cdot x_i +b )\geq 1-\xi_i, i=1,2,...,N \\
& \xi_i \geq 0,i=1,2,...,N
\end{aligned}
$$
