# 逻辑斯蒂回归|机器学习方法（李航）

在感知机中，我们知道一个超平面将特征空间分成两个部分，实例在不同的子空间中则被分为相对应的类。但是存在一个问题在于，我们不知道一个新输入的实例，它属于一个类的概率是多少。例如新输入的实例非常接近超平面，它被分为A类的概念为51%，分为B类的概念为49%。在感知机中将它分为了A类，但是为给出概念。

为了得到这一概率，我们引入了Sigmoid函数：
$$
sigmoid(x)=\frac{1}{1+e^{-x}}
$$
Sigmoid函数能够将线性回归产生的值$(-\infty，+\infty)$转换到$(0,1)$区间内，而概率的取值也在$(0,1)$内，这样，就可以显示一个实例被分为一个类的概率是多少了。



# 二项逻辑斯谛回归

## 公式推导

逻辑斯蒂函数的一般形式，其分布具有以下分布函数和密度函数：
$$
F(x)=P(X\leq x) =\frac{1}{1+e^{-(x-u)/r}}
$$

$$
f(x)=F'(x)=\frac{1}{(1+e^{-(x-u)/r})^2}\times e^{-(x-u)/r}\times\frac{1}{r}=\frac{e^{-(x-u)/r}}{r(1+e^{-(x-u)/r})^2}
$$

式中，μ为位置参数，$\gamma>0$为形状参数。

![image-20240202175002687](G:\workspaces\machine-learningAlgorithms\逻辑斯蒂回归\images\6.1.png)

分布函数以（μ,1/2）为中心对称，满足：
$$
F(-x-\mu)-\frac{1}{2}=-F(x+\mu)+\frac{1}{2}
$$
形状参数$\gamma$的值越小，分布函数曲线在中心附近增长得越快。

现在，我们让$\mu$取0，$\gamma$取1，即得到我们在逻辑斯蒂回归中使用的函数：
$$
F(x)=\frac{1}{1+e^{(-x)}}=\frac{1+e^x}{e^x}
$$
采用上式，我们将线性回归产生的值代入到sigmoid函数之中，可得：
$$
P(Y=1|x)=\frac{1}{1+exp(-(w*x)+b)}
$$

$$
P(Y=0|x)=1-P(Y=1|x)=1-\frac{1}{1+exp(-(w*x)+b)}=\frac{exp(-(w*x)+b)}{1+exp(-(w*x)+b)}
$$

二项逻辑斯谛回归模型是一种分类模型，由条件概率分布$P(Y|X)$表示。这里，随机变量x取值为实数，随机变量$\gamma$取值为1或0。

这样，我们就将范围为实数的线性回归产生的值转变为逻辑斯蒂回归中仅在$(0,1)$范围之内。

逻辑斯谛回归仅对二分类的问题有效，我们可以比较$P(Y=1|x)$和$P(Y=0|x)$两个条件概率值的大小，将实例x分到概率较大的那一类，同时也能得知分成两种类别的可能性是多少。



## 逻辑斯蒂回归与几率

一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$ ，该事件的对数几率或$logit$函数是：
$$
logit(p)=log\frac{p}{1-p}
$$
我们将逻辑斯蒂回归的$P$代入，可得：
$$
log\frac{P(Y=1|x)}{1-P(Y=1|x)}=log\frac{\frac{exp(-(w*x)+b)}{1+exp(-(w*x)+b)}}{1-\frac{exp(-(w*x)+b)}{1+exp(-(w*x)+b)}}=w*x+b
$$
通过上式我们知道，通过几率的概念对线性函数进行转换，可以得到逻辑斯蒂回归公式。

一个直观的理解是，对于上式，分子是$y=1$的概率，而分母是$y≠1$的概率，显然$wx+b$越大，$y=1$的概率越大，也就是实例点$x$在$y=1$的一侧距离分离超平面越远，则$y=1$的概率越大。



## 模型参数估计

设：
$$
P(Y=1|x)=\pi(x),P(Y=0|x)=1-\pi(x)
$$
似然函数为：
$$
\prod_{i=1}^N[\pi(x_i)]^{y_i}
$$
为了计算方便，我们对似然函数取对数，得到对数似然函数：
$$
\begin{aligned}
L(w)
& =\sum_{i=1}^N[y_ilog\pi (x_i)+(i-y_i)log(1-\pi(x_i))] \\
& =\sum_{i=1}^N[y_ilog\frac{\pi(x_i)}{1-\pi(x_i)}+log(1-\pi(x_i))] \\
& =\sum_{i=1}^N[y_i(w*x_i)-log(1+exp(w*x_i))]
\end{aligned}
$$
对$L(w)$求极大值，可以得到$w$的估计值。



# 多项逻辑斯蒂回归

多项逻辑斯蒂回归：
$$
P(Y=k|x)=\frac{exp(w_k*x)}{1+\sum_{k=1}^{K-1}exp(w_k*x)},k=1,2,...,K-1
$$

$$
P(Y=k|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(w_k*x)}
$$

参数求解的方法一样可以采用极大似然估计法。



# 损失函数

逻辑斯蒂回归模型的损失函数采用交叉熵损失函数
