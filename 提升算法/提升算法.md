# 提升(boosting)方法

分类问题的提升方法的思想是，学习多个分类器，各个分类器中训练样本的权重是不同的，最后将这些分类器进行线性组合，形成一个最终的分类器，以提高分类的性能。

这种思路类似于将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。

在提升方法中有两个基础概念：强可学习，弱可学习。

简单来说，一个概念如果存在一个算法能够学习它，且正确率很高，那么就称这个概念是强可学习的。对应的，一个概念如果存在一个算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。

然而在之后算法的发展中，强可学习与弱可学习被证明是等价的，那么问题就是，如果已经发现了一个“弱学习算法”，可否将其提升(boost)为“强学习算法”。



# Adaboost算法
接下来我们仅考虑分类问题。基本思路就是通过反复学习，得到一系列弱分类器（又称为基本分类器），然后线性组合这些弱分类器，构成一个强分类器。

很多提升算法所使用的方法，就是对分类器进行迭代，每一次迭代都赋予训练数据集中的实例新的权值，使下一次学习得到的模型能够进化，最后，对得到的所有模型进行线性组合，得到最终的强分类器。

于是，这一问题又转变为以下两个问题

- 应该按照什么样的规则赋予实例权值
- 以及如何组合多个弱分类器来得到强分类器

在Adaboost算法中，每一轮都要提高被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。

至于第2个问题，即弱分类器的组合， Adaboost采取加权多数表决的方法。具体地说，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。这样分类错误的可能性就被降低了。

## Adaboost算法理论

在Adaboost算法中，输入的是训练集和迭代次数M，输出的是最终分类器$G(x)$，即提升后的强学习算法。

之前提到我们需要对训练数据分配权值，在初始化中，我们使各个实例权值相等，这样可以保证在原始数据集上学习得到基本分类器$G_1(x)$：
$$
D_1 = (w_{11},\cdots,w_{1i},\cdots,w_{1N}),w_{1i}=\frac1N,i=1,2,\cdots,N
$$
其中下标1表示是初始权值，D代表权值向量，N代表样本容量，在经过第一轮的学习后将权值更新，下标将+1。学习轮数用m表示。

对于$m = 1, 2, …, M$，我们使用权值具有Dm的训练数据集进行学习，得到对应的基本分类器$G_m(x)$：
$$
G_m(x): \quad x \rightarrow\{-1,+1\}
$$
为了更新数据集实例的权值，需要对每一轮$m = 1, 2, …, M$计算分类器$G_m(x)$在训练数据集上的分类误差率：
$$
e_m=\sum_{i=1}^NP(G_m(x_i) \ne y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\ne y_i)
$$
其中I是指示函数，根据上式可以看到， $e_m$为被误分类的实例的权值之和。

在强分类器中，需要对各个弱分类器添加权重，于是需要计算弱分类器$G_m(x)$的权重系数，用来表示该分类器在最终分类器中的重要程度：
$$
a_m=\frac12 \log\frac{1-e_m}{e_m}
$$
其中对数为自然对数。可以看到，当 $e_m \leq 0.5$时，$ a_m \geq 0$，且 $a_m$随着$e_m$的减少而增大，这样就能让分类误差越小的基本分类器在强分类器中越重要。

为了进行下一轮的学习，需要对训练集实例的权重进行更新，其中被上一轮学习错误分类的实例的权重将会增加，而被正确分类的实例的权重将会下降，因此下一轮的学习中，分类器将更多地考虑之前被分类错误的实例。

新的训练集权重为：
$$
D_{m+1}=(w_{m+1，1}, \cdots, w_{m+1，i}, \cdots, w_{m+1，N})
$$
其中每一个实例的权重为：
$$
w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-a_my_iG_m(x_i)), i=1,2,\cdots,N
$$
$Z_m$是规范化因子：
$$
Z_m=\sum_{i=1}^Nw_{mi}exp(-a_my_iG_m(x_i))
$$
$Z_m$的存在可以让所有实例权值的和为1。

可以看到在样本权重更新的过程中，如果分类错误，在$-a_my_iG_m(x_i)$ 中， $y_iG_m(x_i)$为-1，则原权重将增加，如果分类正确， $y_iG_m(x_i)$为+1，则原权重将减少。

在经过M轮的迭代后，我们将得到M个基本分类器$G_m(x)，m=1,2,…,M$，现在，为了得到最终的强分类器，我们需要对这些弱分类器进行线性组合，即给予它们在强分类器中的权重：
$$
f(x)=\sum_{m=1}^Ma_mG_m(x)
$$


于是最终分类器为：
$$
G(x)=sign(f(x))=sign\left( \sum_{m=1}^Ma_mG_m(x) \right)
$$

## 前向分步算法

Adaboost算法还有另一个解释，即可以认为 Adaboost算法是，模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。

首先来看加法模型。

考虑一个加法模型的一般形式：
$$
f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。显然，对应到Adabosst，它的模型 $f(x)=\sum_{m=1}^Ma_mG_m(x)$ 是一个加法模型。

加法模型的损失函数为：
$$
\sum_{i=1}^NL\left( y_i,\sum_{m=1}^M\beta_mb(x;\gamma_m) \right)
$$
即使用加法模型进行预测，并与真实值比较，根据损失函数L得到该样本的损失，并将所有样本的损失相加，即得到加法模型的损失函数。

最小化这种形式的损失函数是一个十分复杂的问题，于是提出前向分步算法解决这一问题。前向分步算法的思路是：既然对整个加法模型求最小化损失函数的解比较困难，那么我们可以从前向后，每一步只学习一个基函数和它的系数，这样就可以简化这一问题：
$$
\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,\beta b(x_i;\gamma))
$$
从形式上可以看到，上式将下标m去掉了，此时仅使用一个基分类器进行预测，得到的损失函数即该基分类器的损失函数，此时通过最小化这一损失函数，可以优化该基分类器。

具体到Adaboost，就是从前往后对每一轮迭代得到的弱分类器进行最小化损失函数的计算，得到在该轮中最优的弱分类器，再根据这个最优的弱分类器对训练集实例的权值进行迭代。

前向分步算法的基本步骤为：

首先初始化$f_0(x)=0$。

接着，对于每一轮迭代$m = 1, 2, \cdots, M$极小化损失函数：
$$
(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
$$
通过对损失函数的极小化，我们能够得到每一轮的最优参数 $(\beta_m,\gamma_m)$。得到最优参数后即得到在该轮内的最优弱分类器：
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
$$
经过M轮迭代后，得到M个弱分类器，并且每个分类器的损失函数已经最小化，得到了最优参数，再将各个分类器进行线性组合得到加法模型，即强分类器：
$$
f(x)=f_{m}(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
现在具体到Adaboost，可以说，当前向分步算法的损失函数为指数损失函数时，就与Adaboost算法等价了。下面对这一论述进行数学证明。

首先写出前向分步算法的指数损失函数的形式：
$$
L(y,f(x))=exp[-yf(x)]
$$
在Adaboost算法中，对于第m轮迭代得到的$a_m$，$G_m(x)$和$f_m(x)$有下列关系式：
$$
f_m(x)=f_{m-1}(x)+a_mG_m(x)
$$
其中：
$$
f_m(x)=f_{m-2}(x)+a_{m-1}G_{m-1}(x)=a_1G_1(x)+\cdots+a_{m-1}G_{m-1}(x)
$$
现在我们利用前提条件，即损失函数使用指数损失函数，下式成立：
$$
(a_m,G_m(x))=\arg \min_{a,G}\sum_{i=1}^N\exp[-y_i(f_m(x_i))]=\arg\min_{a,G}\sum_{i=1}^N\exp[-y(f_{m-1})+aG(x_i)]
$$
上式最左是指数损失函数最小时弱分类器Gm(x)和弱分类器的系数αm，最右的α和G(x)是还没有最小化损失函数时的分类器和系数，此时它们不是最优的。