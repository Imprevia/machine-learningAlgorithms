# 提升(boosting)方法

分类问题的提升方法的思想是，学习多个分类器，各个分类器中训练样本的权重是不同的，最后将这些分类器进行线性组合，形成一个最终的分类器，以提高分类的性能。

这种思路类似于将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。

在提升方法中有两个基础概念：强可学习，弱可学习。

简单来说，一个概念如果存在一个算法能够学习它，且正确率很高，那么就称这个概念是强可学习的。对应的，一个概念如果存在一个算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。

然而在之后算法的发展中，强可学习与弱可学习被证明是等价的，那么问题就是，如果已经发现了一个“弱学习算法”，如何将其提升(boost)为“强学习算法”，便成为开发提升方法时所要解决的问题。



# Adaboost算法
仅考虑分类问题，Adaboost算法的基本思路就是通过反复学习，得到一系列弱分类器（又称为基本分类器），然后线性组合这些弱分类器，构成一个强分类器。

很多提升算法所使用的方法，就是对分类器进行迭代，每一次迭代都赋予训练数据集中的实例新的权值，使下一次学习得到的模型能够进化，最后，对得到的所有模型进行线性组合，得到最终的强分类器。

于是，这一问题又转变为以下两个问题

- 每一次迭代按照什么样的规则赋予实例权值
- 如何组合多个弱分类器来得到强分类器

在Adaboost算法中，每一轮都要提高被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。

至于第2个问题，即弱分类器的组合， Adaboost采取加权多数表决的方法。具体地说，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。这样分类错误的可能性就被降低了。

## Adaboost算法理论

在Adaboost算法中，输入的是训练集和迭代次数M，输出的是最终分类器$G(x)$，即提升后的强学习算法。

之前提到我们需要对训练数据分配权值，在初始化中，我们使各个实例权值相等，这样可以保证在原始数据集上学习得到基本分类器$G_1(x)$：
$$
D_1 = (w_{11},\cdots,w_{1i},\cdots,w_{1N}),w_{1i}=\frac1N,i=1,2,\cdots,N
$$
其中下标1表示是初始权值，D代表权值向量，N代表样本容量，在经过第一轮的学习后将权值更新，下标将+1。学习轮数用m表示。

对于$m = 1, 2, …, M$，我们使用权值具有Dm的训练数据集进行学习，得到对应的基本分类器$G_m(x)$：
$$
G_m(x): \quad x \rightarrow\{-1,+1\}
$$
为了更新数据集实例的权值，需要对每一轮$m = 1, 2, …, M$计算分类器$G_m(x)$在训练数据集上的分类误差率：
$$
e_m=\sum_{i=1}^NP(G_m(x_i) \ne y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\ne y_i)
$$
其中I是指示函数，根据上式可以看到， $e_m$为被误分类的实例的权值之和。

在强分类器中，需要对各个弱分类器添加权重，于是需要计算弱分类器$G_m(x)$的权重系数，用来表示该分类器在最终分类器中的重要程度：
$$
a_m=\frac12 \log\frac{1-e_m}{e_m}
$$
其中对数为自然对数。可以看到，当 $e_m \leq 0.5$时，$ a_m \geq 0$，且 $a_m$随着$e_m$的减少而增大，这样就能让分类误差越小的基本分类器在强分类器中越重要。

为了进行下一轮的学习，需要对训练集实例的权重进行更新，其中被上一轮学习错误分类的实例的权重将会增加，而被正确分类的实例的权重将会下降，因此下一轮的学习中，分类器将更多地考虑之前被分类错误的实例。

新的训练集权重为：
$$
D_{m+1}=(w_{m+1，1}, \cdots, w_{m+1，i}, \cdots, w_{m+1，N})
$$
其中每一个实例的权重为：
$$
w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-a_my_iG_m(x_i)), i=1,2,\cdots,N
$$
$Z_m$是规范化因子：
$$
Z_m=\sum_{i=1}^Nw_{mi}exp(-a_my_iG_m(x_i))
$$
$Z_m$的存在可以让所有实例权值的和为1。

可以看到在样本权重更新的过程中，如果分类错误，在$-a_my_iG_m(x_i)$ 中， $y_iG_m(x_i)$为-1，则原权重将增加，如果分类正确， $y_iG_m(x_i)$为+1，则原权重将减少。

在经过M轮的迭代后，我们将得到M个基本分类器$G_m(x)，m=1,2,…,M$，现在，为了得到最终的强分类器，我们需要对这些弱分类器进行线性组合，即给予它们在强分类器中的权重：
$$
f(x)=\sum_{m=1}^Ma_mG_m(x)
$$


于是最终分类器为：
$$
G(x)=sign(f(x))=sign\left( \sum_{m=1}^Ma_mG_m(x) \right)
$$

## 前向分步算法

Adaboost算法还有另一个解释，即可以认为 Adaboost算法是，模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。

首先来看加法模型。

考虑一个加法模型的一般形式：
$$
f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。显然，对应到Adabosst，它的模型 $f(x)=\sum_{m=1}^Ma_mG_m(x)$ 是一个加法模型。

加法模型的损失函数为：
$$
\sum_{i=1}^NL\left( y_i,\sum_{m=1}^M\beta_mb(x;\gamma_m) \right)
$$
即使用加法模型进行预测，并与真实值比较，根据损失函数L得到该样本的损失，并将所有样本的损失相加，即得到加法模型的损失函数。

最小化这种形式的损失函数是一个十分复杂的问题，于是提出前向分步算法解决这一问题。前向分步算法的思路是：既然对整个加法模型求最小化损失函数的解比较困难，那么我们可以从前向后，每一步只学习一个基函数和它的系数，这样就可以简化这一问题：
$$
\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,\beta b(x_i;\gamma))
$$
从形式上可以看到，上式将下标m去掉了，此时仅使用一个基分类器进行预测，得到的损失函数即该基分类器的损失函数，此时通过最小化这一损失函数，可以优化该基分类器。

具体到Adaboost，就是从前往后对每一轮迭代得到的弱分类器进行最小化损失函数的计算，得到在该轮中最优的弱分类器，再根据这个最优的弱分类器对训练集实例的权值进行迭代。

前向分步算法的基本步骤为：

首先初始化$f_0(x)=0$。

接着，对于每一轮迭代$m = 1, 2, \cdots, M$极小化损失函数：
$$
(\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
$$
通过对损失函数的极小化，我们能够得到每一轮的最优参数 $(\beta_m,\gamma_m)$。得到最优参数后即得到在该轮内的最优弱分类器：
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
$$
经过M轮迭代后，得到M个弱分类器，并且每个分类器的损失函数已经最小化，得到了最优参数，再将各个分类器进行线性组合得到加法模型，即强分类器：
$$
f(x)=f_{m}(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
$$
现在具体到Adaboost，可以说，当前向分步算法的损失函数为指数损失函数时，就与Adaboost算法等价了。下面对这一论述进行数学证明。

首先写出前向分步算法的指数损失函数的形式：
$$
L(y,f(x))=exp[-yf(x)]
$$
在Adaboost算法中，对于第m轮迭代得到的$a_m$，$G_m(x)$和$f_m(x)$有下列关系式：
$$
f_m(x)=f_{m-1}(x)+a_mG_m(x)
$$
其中：
$$
f_m(x)=f_{m-2}(x)+a_{m-1}G_{m-1}(x)=a_1G_1(x)+\cdots+a_{m-1}G_{m-1}(x)
$$
现在我们利用前提条件，即损失函数使用指数损失函数，下式成立：
$$
(a_m,G_m(x))=\arg \min_{a,G}\sum_{i=1}^N\exp[-y_i(f_m(x_i))]=\arg\min_{a,G}\sum_{i=1}^N\exp[-y(f_{m-1})+aG(x_i)]
$$
上式最左是指数损失函数最小时弱分类器Gm(x)和弱分类器的系数αm，最右的α和G(x)是还没有最小化损失函数时的分类器和系数，此时它们不是最优的。

我们令$\overrightarrow{w}_{mi}=exp[-y_if_{m-1}(x_i)]$，于是上式可以转变为：
$$
(a_m,G_m(x))=\arg\min_{a,G}\sum_{i=1}^N$\overrightarrow{w}_{mi}\exp[-y_iaG(x_i)]
$$
因为$\overrightarrow{w}_{mi}$ 不依赖α也不依赖分类器G，因此与最小化无关，但是它依赖于之前所有得到的弱分类器的线性组合$f_{m-1}(x)$，因此它的值会随着每一轮迭代而变化。

现在分别对$a_m$和$G_m(x)$进行讨论，对于$G_m(x)$而言，我们看到当分类正确，指数为负，当分类错误，指数为正，即当分类错误的实例越少，则损失函数越小，于是可以用指数函数I表示，因此m轮最优基本分类器$G_m^*(x)$可以表示为：
$$
G_m^*(x)=\arg \min_G\sum_{i=1}^N\overrightarrow{w}_{mi}I(y_i \ne G(x_i))
$$
接下来再看$a_m$。
$$
\begin{aligned}
& \sum_{i=1}^N\overrightarrow{w}_{mi}exp[-y_iaG(x_i)] \\
=&\sum_{y_i=G_m(x_i)}\overrightarrow{w}_{mi}e^{-a}+\sum_{y_i\ne G_m(x_i)}\overrightarrow{w}_{mi}e^{a} \\
=&\sum_{y_i=G_m(x_i)}\overrightarrow{w}_{mi}e^{-a}+
\sum_{y_i\ne G_m(x_i)}\overrightarrow{w}_{mi}e^{a}-
\sum_{y_i \ne G_m(x_i)}\overrightarrow{w}_{mi}e^{-a}+
\sum_{y_i\ne G_m(x_i)}\overrightarrow{w}_{mi}e^{a} \\
=& \sum_{y_i\ne G_m(x_i)}\overrightarrow{w}_{mi}e^{a}-
\sum_{y_i \ne G_m(x_i)}\overrightarrow{w}_{mi}e^{-a}+
\sum_{y_i=G_m(x_i)}\overrightarrow{w}_{mi}e^{-a}+
\sum_{y_i\ne G_m(x_i)}\overrightarrow{w}_{mi}e^{a} \\
=& (e^a-e^{-a})\sum_{y_i\ne G_m(x_i)}\overrightarrow{w}_{mi}+e^{-a}\sum_{i}\overrightarrow{w}_{mi} \\
=&(e^a-e^{-a})\sum_{i=1}\overrightarrow{w}_{mi}I(y \ne G(x_i))+e^{-a}\sum_{i=1}^N\overrightarrow{w}_{mi}
\end{aligned}
$$
根据之前Adaboost的推导，可知 $\sum_{i=1}\overrightarrow{w}_{mi}I(y \ne G(x_i))=e_m$ ，且 $\sum_{i=1}^N\overrightarrow{w}_{mi}=1$ ，因此上式可继续转变为：
$$
(e^a-e^{-a})e_m+e^{-a}
$$
现在对其进行关于$a$的求导并令导数为0：
$$
\frac{\partial[(e^a-e^{-a})e_m+e^{-a}]}{\partial a}=e^ae_m+e^{-a}e_m-e^{-a}=0 \\
e^ae_m=e^{e^-a}(1-e_m) \rightarrow \\
\frac{e^a}{e^{-a}}=\frac{1-e_m}{e_m} \rightarrow \\
e^{2a}=\frac{1-e_m}{e_m} \rightarrow \\
2a=\log\frac{1-e_m}{e_m} \rightarrow \\
a= \frac12 \log\frac{1-e_m}{e_m}
$$
于是得到最优$a_m^*$，这里得到的最优$a_m^*$与之前Adaboost推导中得到的基本分类器的系数等式一致。

这样就在前向分步算法中得到的最优$G_m$与$G_m$的系数$a$，和在Adaboost算法中得到的一致。最后再来看一下前向分步算法中每一轮样本权值的更新：

因为：
$$
\overrightarrow{w}_{mi} = \exp[-y_if_{m-1}(x_i)]
$$
所以：
$$
\begin{aligned}
\overrightarrow{w}_{m+1,i}&=\exp[-y_if_{m}(x_i)] \\
&=\exp[-y_if_{m-1}(x_i)+a_mG_m(x)] \\
&=\exp[-y_if_{m-1}(x_i)+y_ia_mG_m(x)] \\
&=\exp[-y_ia_mG_m(x)] \\
\end{aligned}
$$
这与Adaboost算法中只相差规范化因子，而规范化因子并不影响权值的分布，因此两者等价。



## 前向分步算法与Adaboost算法的关系

我们之前说到在前向分步算法中，如果损失函数取指数损失函数，则与Adaboost算法等价，也就是说，Adaboost算法是前向分步算法的一个特殊情况。



# 提升树

提升树是以分类树或回归树为基本分类器的提升方法。

提升树模型可以表示为决策树的加法模型：
$$
f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
$$
其中$T(x;\Theta_m)$ 表示决策树， $\Theta_m$为决策树参数，$M$为树的个数。

## 提升树算法

采用前向分布算法，其初始提升树为：$f_0(x)=0$ ，第$m$步的模型是：
$$
f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
$$
其中 $f_{m-1}(x)$ 是$m-1$步时的强分类器，$T(x;\Theta_m)$是第$m$步得到的新弱分类器，两者的线性组合$f_m(x)$即为新的强分类器。

可以通过经验风险极小化确定下一课决策树的参数：
$$
\hat{\Theta}_m=\arg \min_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
$$
下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。

对于二分类问题，提升树算法只需将Adaboost算法中的基本分类器限制为二分类树即可。

在提升树算法中，基本分类器可表示为：
$$
T(x;\Theta_m) = \sum_{j=1}^Jc_jI(x \in R_j)
$$
其中$R_j$是输入空间中的一个被划分出来的区域，$c_j$代表这个区域的输出值，J代表区域的数量，在树中即是叶节点的数量。参数$\Theta=\{ (R_1,c_1), (R_2,c_2), \dots, (R_J,c_J) \}$，表示树的区域划分和各区域上的常数。

总结一下，回归问题提升树的前向分步算法为：
$$
\begin{aligned}
&f_0(x)=0 \\
&f_m(x)=f_{m-1}(x)+T(x;\Theta_m),m=1,2,\cdots,M \\
&f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
\end{aligned}
$$
在每一步中，都需要最优化当前模型：
$$
\hat{\Theta}_m=\arg \min_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
$$
当上式中的损失函数采用平方误差损失函数时：
$$
L(y,f(x))=(y-f(x))^2
$$
损失变为：
$$
L(y,f_{m-1}(x)+T(x_i;\Theta_m)))=[y-f_{m-1}(x)-T(x_i;\Theta_m))]^2=[r-T(x_i;\Theta_m))]^2
$$
其中 $r=y-f_{m-1}(x)$是当前模型拟合数据的残差。对回归问题的提升树算法来说，只需简单地拟合当前模型的残差。

上句话的意思是，我们可以将上一步得到的训练集实例的残差作为训练集实例的新标记，并用新的标记训练下一颗树。



## 梯度提升

在提升树算法中，当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，则没有那么容易，为了解决这个问题，提出了梯度提升算法。这是一种利用梯度下降法的方法，即在求每一个树的最小化损失函数时，采用梯度下降法得到最优参数。
$$
-\left[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}\right]_{f(x)=f_{m-1}(x)}
$$


梯度提升的关键在于，我们使用损失函数的负梯度在当前模型的值作为训练下一个基本分类器的标记，拟合一个回归树。